\chapter{Stato Dell'Arte}
\label{ch:capitolo7}


\section{Errore apparente}
\label{sec:sezione7.1}


Prima della nascita degli stimatori plug-in, lo stimatore usato per l’errore di previsione era l’errore apparente, o errore di sostituzione. Questa stima, che utilizza gli stessi dati sia per allenare il modello che per calcolarne l’errore, risulta essere distorta positivamente, soprattutto quando i dati a disposizione sono pochi. Questo avviene perché il modello è testato sugli stessi dati che ha visto durante l'allenamento, portando a una stima eccessivamente ottimistica delle sue performance. Una semplice soluzione a questo problema è quella di dividere il dataset iniziale in due, assegnando un set di dati alla selezione del modello e un altro alla valutazione del modello. Questi set di dati sono comunemente chiamati training set e test set. Tuttavia, questo metodo soffre di problematiche relative alla variabilità della stima quando il dataset è di piccole dimensioni. Quando il dataset è ridotto, la parte riservata alla valutazione può non essere rappresentativa dell'intero dominio dei dati, causando una stima inaccurata delle performance del modello. Inoltre, la variabilità tra le possibili partizioni può portare a stime dell'errore di previsione molto diverse a seconda della partizione scelta. Questo problema è amplificato nei contesti ad alta dimensionalità, dove il numero di variabili supera di gran lunga il numero di osservazioni, come spesso accade nei dati di microarray (\textcite{mclachlan}).\\
Per affrontare questi problemi, tecniche più sofisticate, come la cross-validation e il bootstrap, sono state proposte e hanno guadagnato accettazione come standard nel campo della statistica e del machine learning. Queste tecniche, discusse più avanti, offrono soluzioni più robuste e affidabili per la stima dell'errore di previsione.


\section{Cross Validation}
\label{sec:sezione7.2}


Una delle prime proposte per uno stimatore che massimizzasse l’utilizzo delle informazioni disponibili, evitando di escludere troppe osservazioni per formare un test set ma che fosse anche unbiased, è stato il metodo leave-one-out (LOO). Questo metodo prevede di escludere una sola osservazione dal training set, sul quale il modello viene poi valutato. Tuttavia questo metodo introduce una grande variabilità nella stima dell’errore di previsione, poiché il modello può ottenere un’accuratezza stimata o del 100\% o dello 0\% a seconda dell’osservazione esclusa.\\
Per risolvere questo problema, \textcite{lachenbruch1968} hanno proposto una tecnica oggi conosciuta come leave-one-out cross-validation (LOOCV). Questa tecnica consiste nel ripetere il metodo leave-one-out per tutte le osservazioni nel dataset, calcolando poi la media degli errori ottenuti per stimare l’errore di previsione finale. \textcite{stone} ha ulteriormente sviluppato il concetto di cross-validation, discutendone l’uso per la valutazione delle previsioni statistiche e ponendo importanti basi per studi successivi. Per diminuire la potenza di calcolo richiesta dalla LOOCV, viene introdotta la $k$-fold cross-validation, che consiste nel dividere il dataset in $k$ parti uguali, per poi allenare il modello su $k-1$ parti e testarlo sulla parte rimanente. Questo processo viene ripetuto $k$ volte, ogni volta con una diversa parte come set di test, e la stima finale dell’errore di previsione è la media degli errori ottenuti. Questo metodo bilancia meglio il bias e la varianza rispetto al leave-one-out, riducendo la varianza senza aumentare significativemente il bias.\\
\textcite{wongTT} valuta la performance di algoritmi di classificazione utilizzando queste due tecniche di cross-validation, evidenziando i vantaggi e gli svantaggi di ciascuna tecnica a seconda dello scenario applicativo. \textcite{mclachlan} conducono uno studio su dati di tipo microarray ed ottengono risultati in favore della 10-fold cross-validation rispetto alla leave-one-out, dato che la prima offre un ottimo bilanciamento tra bias e varianza, mentre la seconda potrebbe presentare un basso bias ma a costo di un’alta varianza. \textcite{celisse} hanno condotto una revisione completa delle procedure di cross-validation per la selezione del modello, evidenziando i punti di forza e le debolezze di ciascun metodo. Il loro lavoro è fondamentale per comprendere l'evoluzione delle tecniche di cross-validation e le loro applicazioni pratiche.


\section{Bootstrap}
\label{sec:sezione7.3}


\textcite{efron1983} introduce le tecniche basate sul bootstrap per migliorare l’accuratezza della stima dell’errore di previsione, ponendo le basi per sviluppi successivi nell’ambito dei metodi di ricampionamento. Il principale obiettivo dei metodi proposti da Efron è ridurre il bias ottimista associato all’errore apparente. Tra gli stimatori presentati vi sono il bootstrap semplice, il doppio bootstrap e il bootstrap .632. Quest’ultimo, in particolare, combina la stima ottenuta dal bootstrap semplice con l’errore apparente, assegnando un peso di $.632$ al primo e $.368$ al secondo, diminuendo così il bias ottimista dell’errore apparente. Successivamente, \textcite{improvements} propongono il bootstrap .632+, che introduce il concetto di no-information error rate. Questo metodo assegna i pesi ai due errori in base al livello di overfitting, dando maggiore importanza all’errore apparente quando l’overfitting è minore.\\
I libri di \textcite{efron1993} e di \textcite{guideforpractitioners} rappresentano una guida completa sui metodi basati sul bootstrap, offrendo una panoramica dettagliata delle varie tecniche e delle loro applicazioni. \textcite{jiang2007} confrontano diversi stimatori bootstrap per la stima dell’errore di previsione nella classificazione microarray, introducendo due nuovi stimatori: il repeated-leave-one-out bootstrap (RLOOB) e l’adjusted bootstrap (ABS), volti a correggere il bias degli stimatori bootstrap. \textcite{jiang2013} modificano il bootstrap .632+ per risolvere i problemi riscontrati nei dati microarray, dove il numero di variabili supera quello delle osservazioni. L’articolo fornisce sia spunti teorici sia linee guida pratiche per applicare la loro versione modificata del bootstrap .632+. \textcite{bischl} esplorano vari metodi di ricampionamento, tra cui tecniche basate sul bootstrap, per valutare meta-modelli in computazione evolutiva. Gli autori dimostrano come queste tecniche possano migliorare l’affidabilità del modello e fornire una valutazione più accurata.


\section{Metodi di stima alternativi}
\label{sec:sezione7.4}


\textcite{Fuetal} esplorano l'utilizzo del bootstrap combinato con la cross-validation per la stima dell'errore di previsione. Il loro studio mostra che questa combinazione fornisce stime dell'errore più affidabili rispetto ai metodi tradizionali, soprattutto quando il numero di osservazioni è limitato. \textcite{vansanden} evidenziano le possibili distorsioni nella stima dell'errore di previsione in presenza di dati ad alta dimensionalità, proponendo l'uso del bootstrap-cross-validation (BCV) per mitigare tali problemi. \textcite{hefny} introducono un nuovo stimatore dell'errore di previsione basato sul metodo Monte Carlo, fornendo un'alternativa sia alla tradizionale cross-validation sia ai metodi basati sul bootstrap. Il loro studio, che include analisi teoriche e risultati empirici, dimostra l'efficacia del nuovo stimatore che chiamano GMCP (Combined Generative and Monte-Carlo posterior estimates).\\
\textcite{cawley} affrontano il problema dell'overfitting nella selezione del modello e la conseguente distorsione nella valutazione della performance. \textcite{varma} esaminano il bias nella stima dell'errore quando si utilizza la cross-validation per la selezione del modello, sottolineando l'importanza di considerare questo aspetto per evitare stime ottimistiche delle performance del modello.\\
Studi comparativi come quelli di \textcite{wongTT} e \textcite{kim2009} hanno dimostrato che ripetere la  $k$-fold cross-validation riduce la varianza in modo più efficiente rispetto alla semplice $k$-fold cross-validation. Tuttavia questa maggiore efficienza richiede uno sforzo computazionale che aumenta esponenzialmente in base al numero delle osservazioni. Mostrano inoltre come in diversi casi restituisca una stima più accurata del .632+.


\section{Applicazioni pratiche e studi comparativi}
\label{sec:sezione7.5}


Uno dei primi studi comparativi è stato quello di \textcite{Kohavi}, che ha confrontato metodi basati su cross-validation e bootstrap per la stima dell’accuratezza e la selezione del modello. I suoi risultati sono ancora rilevanti nelle tecniche di valutazione dei modelli contemporanee. Kohavi ha mostrato che, sebbene diversi metodi possano essere efficaci, la scelta del metodo appropriato dipende dal contesto specifico e dalle caratteristiche del dataset. Nell’ambito dei dati microarray, caratterizzati dalla ristretta quantità di osservazioni disponibili e dati ad alta dimensionalità, sono stati condotti diversi studi comparativi. \textcite{mclachlan} si concentrano sul bias di selezione nella selezione dei geni con dati di tipo microarray gene-expression. Raccomandano la 10-fold cross-validation rispetto alla leave-one-out e, tra gli stimatori bootstrap, preferiscono il .632+, poiché questo metodo offre un miglior bilanciamento tra bias e varianza.\\
\textcite{molinaro2005} confrontano vari metodi di ricampionamento per stimare l’errore di previsione. Lo studio fornisce un’analisi dettagliata dei punti di forza e di debolezza dei vari metodi, fungendo da guida per il loro utilizzo nella bioinformatica. Gli autori sottolineano che nessun metodo è universalmente migliore, ma che la scelta dipende dalle specifiche caratteristiche del dataset e del problema. Nell’ambito dell’analisi discriminante, \textcite{empirical} e \textcite{Ikechukwu} offrono un confronto empirico di diversi stimatori dell’errore di previsione. I primi analizzano l’analisi discriminante logistica, mentre Ikechukwu esamina l’analisi discriminante con variabili multivariate binarie. I loro studi mostrano come diversi metodi di stima possano influenzare significativamente le performance del modello, suggerendo che la scelta del metodo di stima debba essere attentamente considerata in base al contesto specifico. \textcite{techrep} confrontano vari metodi, inclusi quelli basati su penalità della covarianza. Il loro studio, attraverso estensive simulazioni, fornisce una valutazione delle performance relative degli stimatori. Successivamente, \textcite{diciaccio} confrontano anche vari stimatori per l’errore extra-sample per metodi non parametrici. \\
Sulla base dei risultati ottenuti dai sopracitati studi comparativi, si è deciso di selezionare come oggetto di studio gli stimatori che hanno avuto le migliori prestazioni, andando a considerare anche l'aspetto dell'intensità computazionale richiesta con il fine di valutare se sia giustificata o meno. Nell'ambito delle tecniche basate sul bootstrap sono stati selezionati il bootstrap semplice, il leave-one-out bootstrap, il bootstrap .632 ed il bootstrap .632+. Per quanto concerne invece la cross-validation sono stati selezionati il leave-one-out cross-validation ed il $k$-fold cross-validation. Infine, come stimatori sviluppati più recentemente sono stati selezionati il bootstrap-cross-validation ed il Monte Carlo cross-validation (MCCV).


